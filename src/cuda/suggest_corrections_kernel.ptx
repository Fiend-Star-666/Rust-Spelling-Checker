//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-33567101
// Cuda compilation tools, release 12.3, V12.3.107
// Based on NVVM 7.0.1
//

.version 8.3
.target sm_52
.address_size 64

	// .globl	suggest_corrections_kernel
.global .align 8 .b8 dictionary[80000];
.global .align 4 .u32 dictionary_size = 10000;

.visible .entry suggest_corrections_kernel(
	.param .u64 suggest_corrections_kernel_param_0,
	.param .u64 suggest_corrections_kernel_param_1,
	.param .u32 suggest_corrections_kernel_param_2
)
{
	.local .align 4 .b8 	__local_depot0[80008];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<34>;
	.reg .b16 	%rs<15>;
	.reg .b32 	%r<139>;
	.reg .b64 	%rd<56>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd24, [suggest_corrections_kernel_param_0];
	ld.param.u64 	%rd25, [suggest_corrections_kernel_param_1];
	ld.param.u32 	%r45, [suggest_corrections_kernel_param_2];
	add.u64 	%rd1, %SPL, 0;
	add.u64 	%rd2, %SPL, 40004;
	mov.u32 	%r46, %ntid.x;
	mov.u32 	%r47, %ctaid.x;
	mov.u32 	%r48, %tid.x;
	mad.lo.s32 	%r1, %r47, %r46, %r48;
	setp.ge.s32 	%p1, %r1, %r45;
	@%p1 bra 	$L__BB0_37;

	cvta.to.global.u64 	%rd28, %rd24;
	cvt.s64.s32 	%rd3, %r1;
	mul.wide.s32 	%rd29, %r1, 8;
	add.s64 	%rd30, %rd28, %rd29;
	ld.global.u64 	%rd4, [%rd30];
	ld.global.u32 	%r2, [dictionary_size];
	setp.lt.s32 	%p2, %r2, 1;
	@%p2 bra 	$L__BB0_37;

	cvta.to.global.u64 	%rd31, %rd25;
	shl.b64 	%rd32, %rd3, 3;
	add.s64 	%rd5, %rd31, %rd32;
	mov.u32 	%r124, 2147483647;
	mov.u32 	%r123, 0;
	mov.u64 	%rd34, dictionary;

$L__BB0_3:
	mul.wide.s32 	%rd33, %r123, 8;
	add.s64 	%rd35, %rd34, %rd33;
	ld.global.u64 	%rd6, [%rd35];
	mov.u64 	%rd51, %rd4;

$L__BB0_4:
	mov.u64 	%rd7, %rd51;
	ld.u8 	%rs2, [%rd7];
	setp.ne.s16 	%p3, %rs2, 0;
	add.s64 	%rd51, %rd7, 1;
	@%p3 bra 	$L__BB0_4;

	sub.s64 	%rd36, %rd7, %rd4;
	cvt.u32.u64 	%r5, %rd36;
	mov.u64 	%rd52, %rd6;

$L__BB0_6:
	mov.u64 	%rd9, %rd52;
	ld.u8 	%rs3, [%rd9];
	setp.ne.s16 	%p4, %rs3, 0;
	add.s64 	%rd52, %rd9, 1;
	@%p4 bra 	$L__BB0_6;

	sub.s64 	%rd11, %rd9, %rd6;
	cvt.u32.u64 	%r6, %rd11;
	setp.lt.s32 	%p5, %r6, 0;
	@%p5 bra 	$L__BB0_15;

	cvt.u32.u64 	%r52, %rd9;
	add.s32 	%r53, %r52, 1;
	cvt.u32.u64 	%r54, %rd6;
	sub.s32 	%r7, %r53, %r54;
	and.b32  	%r8, %r7, 3;
	setp.lt.u32 	%p6, %r6, 3;
	mov.u32 	%r127, 0;
	@%p6 bra 	$L__BB0_11;

	sub.s32 	%r126, %r7, %r8;
	mov.u32 	%r127, 0;

$L__BB0_10:
	mul.wide.s32 	%rd37, %r127, 4;
	add.s64 	%rd38, %rd1, %rd37;
	st.local.u32 	[%rd38], %r127;
	add.s32 	%r56, %r127, 1;
	st.local.u32 	[%rd38+4], %r56;
	add.s32 	%r57, %r127, 2;
	st.local.u32 	[%rd38+8], %r57;
	add.s32 	%r58, %r127, 3;
	st.local.u32 	[%rd38+12], %r58;
	add.s32 	%r127, %r127, 4;
	add.s32 	%r126, %r126, -4;
	setp.ne.s32 	%p7, %r126, 0;
	@%p7 bra 	$L__BB0_10;

$L__BB0_11:
	setp.eq.s32 	%p8, %r8, 0;
	@%p8 bra 	$L__BB0_15;

	mul.wide.s32 	%rd39, %r127, 4;
	add.s64 	%rd12, %rd1, %rd39;
	st.local.u32 	[%rd12], %r127;
	setp.eq.s32 	%p9, %r8, 1;
	@%p9 bra 	$L__BB0_15;

	add.s32 	%r59, %r127, 1;
	st.local.u32 	[%rd12+4], %r59;
	setp.eq.s32 	%p10, %r8, 2;
	@%p10 bra 	$L__BB0_15;

	add.s32 	%r60, %r127, 2;
	st.local.u32 	[%rd12+8], %r60;

$L__BB0_15:
	setp.lt.s32 	%p11, %r5, 1;
	@%p11 bra 	$L__BB0_34;

	cvt.u32.u64 	%r62, %rd9;
	cvt.u32.u64 	%r63, %rd6;
	not.b32 	%r64, %r63;
	add.s32 	%r15, %r64, %r62;
	add.s32 	%r65, %r62, 1;
	mov.u32 	%r128, 1;
	sub.s32 	%r66, %r65, %r63;
	and.b32  	%r16, %r66, 3;
	and.b32  	%r17, %r6, 3;
	sub.s32 	%r18, %r6, %r17;
	sub.s32 	%r19, %r66, %r16;

$L__BB0_17:
	mov.u32 	%r20, %r128;
	st.local.u32 	[%rd2], %r20;
	setp.lt.s32 	%p12, %r6, 1;
	@%p12 bra 	$L__BB0_25;

	setp.lt.u32 	%p13, %r15, 3;
	add.s32 	%r68, %r20, -1;
	cvt.s64.s32 	%rd40, %r68;
	add.s64 	%rd41, %rd4, %rd40;
	ld.u8 	%rs1, [%rd41];
	mov.u32 	%r134, 1;
	mov.u32 	%r133, %r20;
	@%p13 bra 	$L__BB0_21;

	ld.local.u32 	%r129, [%rd1];
	mov.u32 	%r134, 1;
	mov.u64 	%rd53, %rd6;
	mov.u64 	%rd54, %rd2;
	mov.u64 	%rd55, %rd1;
	mov.u32 	%r133, %r20;
	mov.u32 	%r132, %r18;

$L__BB0_20:
	ld.u8 	%rs4, [%rd53];
	setp.ne.s16 	%p14, %rs1, %rs4;
	selp.u32 	%r70, 1, 0, %p14;
	ld.local.u32 	%r71, [%rd55+4];
	add.s32 	%r72, %r71, 1;
	add.s32 	%r73, %r133, 1;
	min.s32 	%r74, %r72, %r73;
	add.s32 	%r75, %r129, %r70;
	min.s32 	%r76, %r74, %r75;
	st.local.u32 	[%rd54+4], %r76;
	ld.u8 	%rs6, [%rd53+1];
	setp.ne.s16 	%p15, %rs1, %rs6;
	selp.u32 	%r77, 1, 0, %p15;
	ld.local.u32 	%r78, [%rd55+8];
	add.s32 	%r79, %r78, 1;
	add.s32 	%r80, %r76, 1;
	min.s32 	%r81, %r79, %r80;
	add.s32 	%r82, %r71, %r77;
	min.s32 	%r83, %r81, %r82;
	st.local.u32 	[%rd54+8], %r83;
	ld.u8 	%rs7, [%rd53+2];
	setp.ne.s16 	%p16, %rs1, %rs7;
	selp.u32 	%r84, 1, 0, %p16;
	ld.local.u32 	%r85, [%rd55+12];
	add.s32 	%r86, %r85, 1;
	add.s32 	%r87, %r83, 1;
	min.s32 	%r88, %r86, %r87;
	add.s32 	%r89, %r78, %r84;
	min.s32 	%r90, %r88, %r89;
	st.local.u32 	[%rd54+12], %r90;
	ld.u8 	%rs8, [%rd53+3];
	setp.ne.s16 	%p17, %rs1, %rs8;
	selp.u32 	%r91, 1, 0, %p17;
	add.s64 	%rd16, %rd55, 16;
	ld.local.u32 	%r129, [%rd55+16];
	add.s32 	%r92, %r129, 1;
	add.s32 	%r93, %r90, 1;
	min.s32 	%r94, %r92, %r93;
	add.s32 	%r95, %r85, %r91;
	min.s32 	%r133, %r94, %r95;
	add.s64 	%rd17, %rd54, 16;
	st.local.u32 	[%rd54+16], %r133;
	add.s32 	%r134, %r134, 4;
	add.s64 	%rd53, %rd53, 4;
	add.s32 	%r132, %r132, -4;
	setp.ne.s32 	%p18, %r132, 0;
	mov.u64 	%rd54, %rd17;
	mov.u64 	%rd55, %rd16;
	@%p18 bra 	$L__BB0_20;

$L__BB0_21:
	setp.eq.s32 	%p19, %r17, 0;
	@%p19 bra 	$L__BB0_25;

	setp.eq.s32 	%p20, %r17, 1;
	cvt.s64.s32 	%rd42, %r134;
	add.s64 	%rd19, %rd6, %rd42;
	ld.u8 	%rs9, [%rd19+-1];
	setp.ne.s16 	%p21, %rs1, %rs9;
	selp.u32 	%r96, 1, 0, %p21;
	mul.wide.s32 	%rd43, %r134, 4;
	add.s64 	%rd20, %rd1, %rd43;
	ld.local.u32 	%r32, [%rd20];
	add.s32 	%r97, %r32, 1;
	add.s32 	%r98, %r133, 1;
	min.s32 	%r99, %r97, %r98;
	ld.local.u32 	%r100, [%rd20+-4];
	add.s32 	%r101, %r100, %r96;
	min.s32 	%r33, %r99, %r101;
	add.s64 	%rd21, %rd2, %rd43;
	st.local.u32 	[%rd21], %r33;
	@%p20 bra 	$L__BB0_25;

	setp.eq.s32 	%p22, %r17, 2;
	ld.u8 	%rs11, [%rd19];
	setp.ne.s16 	%p23, %rs1, %rs11;
	selp.u32 	%r102, 1, 0, %p23;
	ld.local.u32 	%r34, [%rd20+4];
	add.s32 	%r103, %r34, 1;
	add.s32 	%r104, %r33, 1;
	min.s32 	%r105, %r103, %r104;
	add.s32 	%r106, %r32, %r102;
	min.s32 	%r35, %r105, %r106;
	st.local.u32 	[%rd21+4], %r35;
	@%p22 bra 	$L__BB0_25;

	ld.u8 	%rs13, [%rd19+1];
	setp.ne.s16 	%p24, %rs1, %rs13;
	selp.u32 	%r107, 1, 0, %p24;
	ld.local.u32 	%r108, [%rd20+8];
	add.s32 	%r109, %r108, 1;
	add.s32 	%r110, %r35, 1;
	min.s32 	%r111, %r109, %r110;
	add.s32 	%r112, %r34, %r107;
	min.s32 	%r113, %r111, %r112;
	st.local.u32 	[%rd21+8], %r113;

$L__BB0_25:
	@%p5 bra 	$L__BB0_33;

	setp.lt.u32 	%p26, %r6, 3;
	mov.u32 	%r137, 0;
	@%p26 bra 	$L__BB0_29;

	mov.u32 	%r137, 0;
	mov.u32 	%r136, %r19;

$L__BB0_28:
	mul.wide.s32 	%rd44, %r137, 4;
	add.s64 	%rd45, %rd2, %rd44;
	ld.local.u32 	%r116, [%rd45];
	add.s64 	%rd46, %rd1, %rd44;
	st.local.u32 	[%rd46], %r116;
	ld.local.u32 	%r117, [%rd45+4];
	st.local.u32 	[%rd46+4], %r117;
	ld.local.u32 	%r118, [%rd45+8];
	st.local.u32 	[%rd46+8], %r118;
	ld.local.u32 	%r119, [%rd45+12];
	st.local.u32 	[%rd46+12], %r119;
	add.s32 	%r137, %r137, 4;
	add.s32 	%r136, %r136, -4;
	setp.ne.s32 	%p27, %r136, 0;
	@%p27 bra 	$L__BB0_28;

$L__BB0_29:
	setp.eq.s32 	%p28, %r16, 0;
	@%p28 bra 	$L__BB0_33;

	setp.eq.s32 	%p29, %r16, 1;
	mul.wide.s32 	%rd47, %r137, 4;
	add.s64 	%rd22, %rd2, %rd47;
	ld.local.u32 	%r120, [%rd22];
	add.s64 	%rd23, %rd1, %rd47;
	st.local.u32 	[%rd23], %r120;
	@%p29 bra 	$L__BB0_33;

	setp.eq.s32 	%p30, %r16, 2;
	ld.local.u32 	%r121, [%rd22+4];
	st.local.u32 	[%rd23+4], %r121;
	@%p30 bra 	$L__BB0_33;

	ld.local.u32 	%r122, [%rd22+8];
	st.local.u32 	[%rd23+8], %r122;

$L__BB0_33:
	add.s32 	%r128, %r20, 1;
	setp.lt.s32 	%p31, %r20, %r5;
	@%p31 bra 	$L__BB0_17;

$L__BB0_34:
	shl.b64 	%rd48, %rd11, 32;
	shr.s64 	%rd49, %rd48, 30;
	add.s64 	%rd50, %rd1, %rd49;
	ld.local.u32 	%r42, [%rd50];
	setp.ge.s32 	%p32, %r42, %r124;
	@%p32 bra 	$L__BB0_36;

	st.global.u64 	[%rd5], %rd6;
	mov.u32 	%r124, %r42;

$L__BB0_36:
	add.s32 	%r123, %r123, 1;
	setp.lt.s32 	%p33, %r123, %r2;
	@%p33 bra 	$L__BB0_3;

$L__BB0_37:
	ret;

}

